{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Prep\n",
    "\n",
    "# import modules and load data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "import os\n",
    "from scipy import stats\n",
    "\n",
    "my_dir = os.path.realpath('.')\n",
    "db_file = os.path.join(my_dir, '../../gr_sentiment_analysis/data/books.db')\n",
    "conn = sqlite3.connect(db_file)\n",
    "\n",
    "#conn = sqlite3.connect('c:/users/nick/onedrive/documents/springboard/sentiment_analysis/gr_sentiment_analysis/data/books.db')\n",
    "review_stats = pd.read_sql_query('SELECT * FROM review_stats',con=conn)\n",
    "\n",
    "# Clean up the data a bit. Remove reviews with a score of 0 since they won't be part of the prediction model \n",
    "\n",
    "review_stats = review_stats[review_stats['rating'] != 0]\n",
    "\n",
    "# Review ID is not needed for this analysis so should be dropped\n",
    "if review_stats.columns.contains('review_id'):\n",
    "    review_stats.drop('review_id', axis=1, inplace=True)\n",
    "\n",
    "    \n",
    "\n",
    "# remove any reviews with a total AFINN or Bing count of 0, since this means there are no matching\n",
    "# words in either lexicon and these reviews cannot be used in this analysis\n",
    "review_stats = review_stats[(review_stats.total_afinn_count != 0) & (review_stats.total_bing_count != 0)]\n",
    "# this leaves 918438 total reviews\n",
    "\n",
    "# remove outliers where the Z-score of the pos or neg word counts is < 3\n",
    "review_stats = review_stats[(\n",
    "    np.abs(stats.zscore(review_stats[['pos_afinn_count', 'neg_afinn_count', 'pos_bing_count', 'neg_bing_count']])) < 3\n",
    ").all(axis=1)]\n",
    "\n",
    "# with all outliers removed (having a count +/- 3 SDs), 889807 rows remain. \n",
    "\n",
    "# Rating is our target variable, the other columns are features. \n",
    "y = review_stats['rating'].values\n",
    "X = review_stats.drop('rating', axis=1).values\n",
    "\n",
    "# Create a training and testing set \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's try a random search using decision trees and see if we get a better result\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy.stats import randint\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# create our pipeline and parameter dict\n",
    "# tree_steps = [('scaler', StandardScaler()),\n",
    "#              ('tree', DecisionTreeClassifier())] \n",
    "\n",
    "param_tree = {'max_depth':[5, 10, 15, 20, None], \n",
    "              'max_features':randint(5,25),\n",
    "              'min_samples_leaf': randint(1,20),\n",
    "              'criterion': ['gini', 'entropy']}\n",
    "\n",
    "tree_cv = RandomizedSearchCV(DecisionTreeClassifier(), param_tree, cv=5, n_iter=20, n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree_cv.fit(X_train, y_train)\n",
    "\n",
    "tree_y_pred = tree_cv.predict(X_test)\n",
    "\n",
    "#stats \n",
    "from sklearn.metrics import classification_report\n",
    "print(tree_cv.best_params_)\n",
    "print(tree_cv.best_score_)\n",
    "print(classification_report(y_test, tree_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.00      0.00      0.00      6664\n",
      "          2       0.00      0.00      0.00     15953\n",
      "          3       0.00      0.00      0.00     40160\n",
      "          4       0.35      0.84      0.50     62340\n",
      "          5       0.45      0.25      0.32     52845\n",
      "\n",
      "avg / total       0.26      0.37      0.27    177962\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=5, max_features=24, min_samples_leaf=4, criterion='gini')\n",
    "tree.fit(X_train, y_train)\n",
    "tree_pred = tree.predict(X_test)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, tree_pred))\n",
    "# terrible. Zero precision on the 1s, 2s, and 3s. Bad results on the 4s and middling on the 5s."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
